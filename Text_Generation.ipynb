{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcygKkEVZBaa"
      },
      "source": [
        "<pre>\n",
        "QUEENE:\n",
        "I had thought thou hadst a Roman; for the oracle,\n",
        "Thus by All bids the man against the word,\n",
        "Which are so weak of care, by old care done;\n",
        "Your children were in your holy love,\n",
        "And the precipitation through the bleeding throne.\n",
        "\n",
        "BISHOP OF ELY:\n",
        "Marry, and will, my lord, to weep in such a one were prettiest;\n",
        "Yet now I was adopted heir\n",
        "Of the world's lamentable day,\n",
        "To watch the next way with his father with his face?\n",
        "\n",
        "ESCALUS:\n",
        "The cause why then we are all resolved more sons.\n",
        "\n",
        "VOLUMNIA:\n",
        "O, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, no, it is no sin it should be dead,\n",
        "And love and pale as any will to that word.\n",
        "\n",
        "QUEEN ELIZABETH:\n",
        "But how long have I heard the soul for this world,\n",
        "And show his hands of life be proved to stand.\n",
        "\n",
        "PETRUCHIO:\n",
        "I say he look'd on, if I must be content\n",
        "To stay him from the fatal of our country's bliss.\n",
        "His lordship pluck'd from this sentence then for prey,\n",
        "And then let us twain, being the moon,\n",
        "were she such a case as fills m\n",
        "</pre>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:34.822914Z",
          "iopub.status.busy": "2022-03-29T12:04:34.822455Z",
          "iopub.status.idle": "2022-03-29T12:04:36.803546Z",
          "shell.execute_reply": "2022-03-29T12:04:36.802868Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:36.807568Z",
          "iopub.status.busy": "2022-03-29T12:04:36.807147Z",
          "iopub.status.idle": "2022-03-29T12:04:38.249435Z",
          "shell.execute_reply": "2022-03-29T12:04:38.248857Z"
        },
        "id": "pD_55cOxLkAb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f05cdde-a609-4a20-8608-609b3c2c9f23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:38.252501Z",
          "iopub.status.busy": "2022-03-29T12:04:38.252311Z",
          "iopub.status.idle": "2022-03-29T12:04:38.257567Z",
          "shell.execute_reply": "2022-03-29T12:04:38.257070Z"
        },
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f80414f3-9651-4d64-d3ef-64951cfb20a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read text file and decode\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:38.265863Z",
          "iopub.status.busy": "2022-03-29T12:04:38.265339Z",
          "iopub.status.idle": "2022-03-29T12:04:38.278801Z",
          "shell.execute_reply": "2022-03-29T12:04:38.278233Z"
        },
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2018ca43-aede-4cd1-914e-07714d65ab3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:39.899395Z",
          "iopub.status.busy": "2022-03-29T12:04:39.899193Z",
          "iopub.status.idle": "2022-03-29T12:04:39.910645Z",
          "shell.execute_reply": "2022-03-29T12:04:39.910120Z"
        },
        "id": "6GMlCe3qzaL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f72d2fbd-6a7f-461a-ccbc-0649d33c4427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "#Create StringLookup layer\n",
        "chars_to_ids = tf.keras.layers.StringLookup(vocabulary=vocab,mask_token=None)\n",
        "ids_to_chars = tf.keras.layers.StringLookup(vocabulary=chars_to_ids.get_vocabulary(),invert=True,mask_token=None)\n",
        "\n",
        "all_ids = chars_to_ids(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "\n",
        "#Until this step, you will obtain a tensorflow dataset of character IDs\n",
        "\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(ids_to_chars(ids).numpy().decode('UTF-8'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define sequence length and example per epoch\n",
        "seq_length = 100\n",
        "example_per_epoch = len(text)//(seq_length+1) \n",
        "\n",
        "sequences = ids_dataset.batch(seq_length+1,drop_remainder=True)\n",
        "\n",
        "for sequence in sequences.take(1):\n",
        "    print(ids_to_chars(sequence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7Ly1A0d_AQW",
        "outputId": "50ea09c9-93c2-4e51-82af-5e9267965cf1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:39.939902Z",
          "iopub.status.busy": "2022-03-29T12:04:39.939404Z",
          "iopub.status.idle": "2022-03-29T12:04:39.953271Z",
          "shell.execute_reply": "2022-03-29T12:04:39.952756Z"
        },
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afd0aee3-2f52-4299-f7c6-efccb369da17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ', shape=(), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "#Function that joins the characters together\n",
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(ids_to_chars(ids), axis=-1)\n",
        "    \n",
        "for sequence in sequences.take(1):\n",
        "    print(text_from_ids(sequence))\n",
        "\n",
        "#Function that creates input and label, label is one step ahead of input\n",
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Print out first batch of input and label\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqmjAuSx_Zh2",
        "outputId": "ef9d85cc-fafa-4a33-9105-49b5d5ae31e2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:39.956023Z",
          "iopub.status.busy": "2022-03-29T12:04:39.955581Z",
          "iopub.status.idle": "2022-03-29T12:04:39.958898Z",
          "shell.execute_reply": "2022-03-29T12:04:39.958364Z"
        },
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset_prefetch = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE,drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#Data preparation is completed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:40.355772Z",
          "iopub.status.busy": "2022-03-29T12:04:40.355346Z",
          "iopub.status.idle": "2022-03-29T12:04:40.359022Z",
          "shell.execute_reply": "2022-03-29T12:04:40.358488Z"
        },
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "#Some hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 256\n",
        "rnn_units = 1024\n",
        "\n",
        "#Create model\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x\n",
        "\n",
        "model = MyModel(len(chars_to_ids.get_vocabulary()),embedding_dim,rnn_units)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:40.362066Z",
          "iopub.status.busy": "2022-03-29T12:04:40.361574Z",
          "iopub.status.idle": "2022-03-29T12:04:40.379324Z",
          "shell.execute_reply": "2022-03-29T12:04:40.378705Z"
        },
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c30fe771-1940-4ca2-a5b7-b4e9d721bb0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[ 0.00488965  0.00068259 -0.00153353 ...  0.00733591 -0.00974749\n",
            "   -0.0007874 ]\n",
            "  [ 0.00972322 -0.00383652  0.00700272 ...  0.0055703  -0.00410091\n",
            "    0.00991068]\n",
            "  [ 0.01258326 -0.01383366  0.0041693  ...  0.00134813 -0.00333804\n",
            "    0.01888752]\n",
            "  ...\n",
            "  [ 0.01108962 -0.00550548 -0.00387047 ...  0.00599941 -0.0080848\n",
            "   -0.00770336]\n",
            "  [ 0.01093642 -0.00764689 -0.01293551 ...  0.00728874  0.00206466\n",
            "   -0.00856296]\n",
            "  [ 0.00045355 -0.00731023  0.00479691 ...  0.01368865  0.0095529\n",
            "    0.00959322]]\n",
            "\n",
            " [[ 0.00126451  0.00193415 -0.00446218 ...  0.00039899 -0.00154395\n",
            "    0.01256501]\n",
            "  [ 0.01072659  0.00094822 -0.01116775 ...  0.00160091 -0.00388591\n",
            "    0.00408083]\n",
            "  [-0.00493678  0.00144258 -0.01021042 ...  0.01258428  0.0064707\n",
            "   -0.0105223 ]\n",
            "  ...\n",
            "  [ 0.00743714  0.00641606 -0.00200697 ... -0.00436386 -0.0149189\n",
            "    0.01352312]\n",
            "  [-0.00624997  0.00427362 -0.00547133 ...  0.00881577  0.00075311\n",
            "   -0.00655191]\n",
            "  [ 0.00418323 -0.01199234 -0.00256209 ...  0.00600938  0.00248669\n",
            "    0.01502134]]\n",
            "\n",
            " [[ 0.00699431 -0.00414907  0.00962741 ...  0.00206201  0.00109338\n",
            "    0.01103918]\n",
            "  [ 0.01101363 -0.01401747  0.00644049 ... -0.00021394 -0.00055712\n",
            "    0.01979602]\n",
            "  [ 0.00519484 -0.00839094 -0.00166877 ... -0.00151126 -0.00080853\n",
            "    0.0224925 ]\n",
            "  ...\n",
            "  [ 0.00043567  0.00936082  0.00263802 ... -0.00242581 -0.00700794\n",
            "    0.01097522]\n",
            "  [ 0.01083188  0.00359767 -0.00678529 ... -0.00054669 -0.00712068\n",
            "    0.00313445]\n",
            "  [ 0.00042457 -0.0036177   0.00867574 ...  0.01161528  0.00753154\n",
            "    0.01508199]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 0.00828051 -0.01247862  0.00056146 ... -0.0003985  -0.00098664\n",
            "    0.01601451]\n",
            "  [ 0.01168482 -0.00762106 -0.00966087 ... -0.00111568 -0.00456391\n",
            "    0.00686792]\n",
            "  [ 0.01241905 -0.01107532 -0.01370829 ... -0.00649648 -0.00545215\n",
            "    0.00261896]\n",
            "  ...\n",
            "  [ 0.01707894 -0.0193375   0.01041442 ... -0.00398478  0.00310796\n",
            "    0.02118389]\n",
            "  [ 0.01248902 -0.01431012 -0.00352569 ...  0.00230227  0.00965481\n",
            "    0.00494796]\n",
            "  [ 0.00870122 -0.0076742  -0.0051722  ...  0.00016448  0.00303015\n",
            "    0.01595416]]\n",
            "\n",
            " [[ 0.00879182  0.0020007  -0.00933291 ...  0.00035685 -0.00503382\n",
            "   -0.00184582]\n",
            "  [ 0.02206718  0.00181428 -0.00328251 ... -0.00819057 -0.01448931\n",
            "   -0.00539139]\n",
            "  [ 0.01263455  0.00774414 -0.00057668 ... -0.00182546  0.01124054\n",
            "    0.01166465]\n",
            "  ...\n",
            "  [-0.00476458  0.00202313 -0.00100379 ... -0.0031034  -0.00316119\n",
            "    0.0149783 ]\n",
            "  [ 0.00793449  0.0003145  -0.00861739 ... -0.00119657 -0.00406717\n",
            "    0.00528403]\n",
            "  [-0.00707167 -0.0253001  -0.00363088 ...  0.01778651 -0.00351242\n",
            "    0.01366239]]\n",
            "\n",
            " [[-0.01546522  0.00224429 -0.00047911 ...  0.00187915 -0.0003094\n",
            "    0.00436142]\n",
            "  [ 0.00012694  0.0031034  -0.00929928 ... -0.00021673 -0.00361499\n",
            "    0.00055173]\n",
            "  [-0.00850789 -0.01325422 -0.00327967 ...  0.00983002  0.00691298\n",
            "   -0.00731683]\n",
            "  ...\n",
            "  [-0.00883831  0.00919254  0.00751641 ...  0.004447    0.01122034\n",
            "   -0.00627719]\n",
            "  [ 0.00478845 -0.00046337 -0.00376433 ...  0.01359886  0.00520321\n",
            "   -0.00840653]\n",
            "  [ 0.00614958  0.0005669  -0.00511113 ...  0.00532278  0.00090032\n",
            "    0.00736911]]], shape=(64, 100, 66), dtype=float32)\n",
            "(64, 100, 66)\n",
            "[12 38 60 48  6  4 15 41 33 45  6 20 62 16 22 32 27 30 59  6 62 46 10 14\n",
            " 50 21 19 40 34 54 57 12  9 58 16 13 53 50 61 37 41 59 55  6 47 29  3 48\n",
            " 14 16 26 38 26 49  2 44 45 61  6 23 63 63 13  6 38  9 31 19 20 14  6 14\n",
            "  6 51 24 32 35 52 21 47 14 19  8 15  7 28 64 10  5 43 22 10 45 15  2 44\n",
            " 33 21  6  1]\n",
            "Input:  b'uited\\nBut with that surname; a good memory,\\nAnd witness of the malice and displeasure\\nWhich thou sho' \n",
            "\n",
            "Output: b\";Yui'$BbTf'GwCISNQt'wg3AkHFaUor;.sC?nkvXbtp'hP!iACMYMj efv'Jxx?'Y.RFGA'A'lKSVmHhAF-B,Oy3&dI3fB eTH'\\n\"\n",
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,022,850\n",
            "Trainable params: 4,022,850\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "for input_batch, target_batch in dataset_prefetch.take(1):\n",
        "    example_batch_prediction = model(input_batch)\n",
        "    print(example_batch_prediction)\n",
        "    print(example_batch_prediction.shape)\n",
        "    sampled_indices = tf.random.categorical(example_batch_prediction[0], num_samples=1)\n",
        "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "    print(sampled_indices)\n",
        "    print(\"Input: \", text_from_ids(input_batch[0]).numpy(),\"\\n\")\n",
        "    print(\"Output:\", text_from_ids(sampled_indices).numpy())\n",
        "\n",
        "model.summary()\n",
        "\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer='adam',loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:40.382353Z",
          "iopub.status.busy": "2022-03-29T12:04:40.381985Z",
          "iopub.status.idle": "2022-03-29T12:04:40.384966Z",
          "shell.execute_reply": "2022-03-29T12:04:40.384453Z"
        },
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"/content/sample_data\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir,\"ckpt_{epoch}\")\n",
        "ckpt_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:40.387668Z",
          "iopub.status.busy": "2022-03-29T12:04:40.387314Z",
          "iopub.status.idle": "2022-03-29T12:04:40.398611Z",
          "shell.execute_reply": "2022-03-29T12:04:40.397967Z"
        },
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c439e18-bf83-4ca1-c8de-acff709782ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 26s 128ms/step - loss: 2.7031\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 24s 127ms/step - loss: 1.9825\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 1.7053\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 24s 129ms/step - loss: 1.5469\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 24s 133ms/step - loss: 1.4488\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa4c32dd110>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#Execute training\n",
        "EPOCH = 5\n",
        "model.fit(dataset_prefetch,epochs=EPOCH,callbacks=[ckpt_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:40.401520Z",
          "iopub.status.busy": "2022-03-29T12:04:40.401220Z",
          "iopub.status.idle": "2022-03-29T12:04:40.413436Z",
          "shell.execute_reply": "2022-03-29T12:04:40.412730Z"
        },
        "id": "QO32cMWu4a06"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states\n",
        "\n",
        "one_step_model = OneStep(model, ids_to_chars, chars_to_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-29T12:04:40.416533Z",
          "iopub.status.busy": "2022-03-29T12:04:40.416053Z",
          "iopub.status.idle": "2022-03-29T12:04:40.419238Z",
          "shell.execute_reply": "2022-03-29T12:04:40.418579Z"
        },
        "id": "9NGu-FkO_kYU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6b3b38-849d-4094-e560-7e21cb574cc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "And as their bed verce, why hear, thus join'd at ome\n",
            "were intent! these I amside the eors'\n",
            "Hast too bewn to me father\n",
            "Nurseing thee both swift foolish man one.\n",
            "\n",
            "BUPTINTA:\n",
            "I know me you! no tears thy head, and he will\n",
            "Holding in a bering tanets;\n",
            "Now on it for him and being connoration\n",
            "Censunath, there is death!\n",
            "\n",
            "EDWARD:\n",
            "Pardon my love, put my brother, thou know'tt that hast to ease,\n",
            "If they have so done.\n",
            "Therefore; in thy daughter, but nifer him was,\n",
            "Here's no bitest hate, no, Even the fime\n",
            "Coriole, but intembed to appleen to prize\n",
            "Ay sit expect auried speer of it; looks underseaven here\n",
            "avacken princely shake longet to here to peril; the dogh shall pley you\n",
            "Do not fardwell, let the pial for ever see\n",
            "A please gar end-mannest to-death.\n",
            "\n",
            "ROMEO:\n",
            "Farsheen stoods? For on heaven call\n",
            "To at Lancaster words in heart,\n",
            "Romeo? die anried, the ending truagh\n",
            "Upon the deaths behed: for nothing on thy strecks,\n",
            "But I am no morin, where thee\n",
            "For Kate, canster horses,\n",
            "To be the stuffician so should with \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.945565223693848\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Text_Generation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}